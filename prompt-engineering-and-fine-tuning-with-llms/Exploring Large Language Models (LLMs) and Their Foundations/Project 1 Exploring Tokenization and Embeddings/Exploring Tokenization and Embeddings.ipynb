{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Choose a text\n",
    "text = \"It is our choices, Harry, that show what we truly are, far more than our abilities.\"\n",
    "\n",
    "# Step 2: Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Step 3: Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**token_ids)\n",
    "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "# Reduce dimensions using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings.numpy())\n",
    "\n",
    "# Step 4: Plot the embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], color='blue')\n",
    "for i, token in enumerate(tokens):\n",
    "    plt.annotate(token, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"Token Embeddings Visualization\")\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Print token analysis\n",
    "print(f\"Total Tokens: {len(tokens)}\")\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
